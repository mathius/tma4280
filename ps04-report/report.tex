\documentclass[12pt]{simple-assignment} % template (c) 2014 Vladimír Štill
\usepackage[czech]{babel}
\usepackage{enumerate}

\setcourse{TMA4280}
\setstudent{Martin Ukrop}
\setassignment{4}
% \setproblemname{problem}

\begin{document}
\noindent
This document serves as a report for problem set 4 in the course TMA4280 Supercomputing, Introduction. The problem set is due on Monday, 2014-02-17.

\section{Serial code}
The \texttt{main()} function iterates through a loop independently computing the desired sum $S_n$ for $n = 2^k$ with $k = 3, \dots, 14$. After each iteration, the difference from the precise value is output along with the corresponding $n$. The value of $\pi$ is computed as \texttt{4*arctan(1)} (due to unstandardized macro \texttt{M\_PI}).

The \texttt{computeSum()} function allocated the whole vector first, and then computes the sum $\Sigma_{i=n}^{1}v(i)$. Summing from the smaller numbers is better due to the limited precision of mantissa inf floating-point number representation.

\section{Parallelization using OpenMP}
OpenMP is utilized for micro-parallelization on 2 places: when initializing values of the vector (i.\,e.\ when computing $v(i)=\frac{1}{i^2}$) and when performing the actual summing of the vector elements. Both these cases call for a static schedule, since the amount of work is the same for all loop iterations. The latter also requires a reduction to prevent race condition for the shared \texttt{sum} variable.

There is a possibility for macro-parallelization of the loop in \texttt{main()} with a dynamic schedule. This, however, would much complicate the further use of both MPI and OpenMP within the \texttt{computeSum()} function (nested threading may not be supported and MPI calls would need to address a particular thread on the other node).

\section{Parallelization using MPI}
According to the assignment, MPI is utilized to computes the sum of the generated vector. The root node, after generating the vector, partitions the work and sends the corresponding parts to individual nodes. 

Although the assignment guarantees the number of nodes equal to the power of 2, the splitting is done in a more general way allowing for arbitrary number of processors (the helper function \texttt{getLocalLength()} computes the length of the partition based on the rank and size of the world communicator). Because the partition lengths may differ, a \texttt{MPI\_Scatterv()} call is used to send the corresponding work. After the summing, the nodes reduce the final sum using a call to \texttt{MPI\_Reduce()}.

As with all MPI usage, the calls to \texttt{MPI\_Init()} and \texttt{MPI\_Finalize()} are necessary at the beginning and end of the program. For precise work partitioning, the utilities for determining the size and rank within the world communicator are necessary (\texttt{MPI\_Comm\_size()} and \texttt{MPI\_Comm\_rank()}).

\section{Combination of MPI and OpenMP}
In the current implementation, OpenMP is also used when generating displacements and lengths prior to \texttt{MPI\_Scatterv()} call. If we used a loop of \texttt{MPI\_Send()} instead of scattering, we could use OpenMP to parallelize the MPI calls, the system allowed it (in such case, \texttt{MPI\_Init\_thread()} would have to be used instead of normal initialization). The overhead of this solution would however be much larger than the gain (MPI would have to use the thread-safe implementation and would be unable to use topology-aware message passing that is used in the scatter operation).

\section{Results precision}
All the calculations are performed using double-precision. However, runs using multiple threads/nodes can theoretically yield slightly different results due to rounding errors an limited mantissa precision. This is caused by the non-deterministic order of threads/processes returning their partial results as well as different work partitioning. Different settings (as required in the assignment) resulted in the differences in about the $12^{th}$ significant decimal digit.

\section{Resource requirements}
For the computation of $S_n$, a single processor would use up to $n \cdot 8B$ of memory (plus constant overhead for control variables). This memory requirement cannot be lowered, since the assignment requires us to generate the whole vector $v$ prior to computation. A multi-processor program utilizing MPI only uses this amount on the root processor (because it is responsible for the vector generation), while other processors only allocate space for their work partition. This is ensured by allocating and initializing the vector in an \texttt{if}-clause satisfied only in the root process. 

In the current implementation, $5 \cdot n$ floating-point operations are needed to generate the vector $v$. For each iteration, there are 4 operation needed to compute $\frac{1}{(i+1) \cdot (i+1)}$ and one more to increment the loop control variable. (Note: We could theoretically get one less by constructing the loop with boundaries higher by one, but this way it's more readable for me as a programmer and the compiler would probably optimize it anyway.) All in all, the number of floating-point operations is linear to the vector length.

In the OpenMP version, the number of operations to compute $S_n$ is still linear to the vector length (linear generation, linear summing). The work may (theoretically) be load-balances on multiple threads and thus finish faster. The MPI version is load-balancing the computation (each processor sums its partition of nearly the same size), but still leaves the memory and computation bottleneck in the process of generating the vector (since this is only done on a single processor).

\section{Conclusions}
Both shared-memory and distributed-memory parallelization seem attractive to solve computationally intensive mathematical problems. However, this problem set was probably too small to really benefit from the parallelization techniques, since the cost of fork/joins (OpenMP usage) and/or process synchronization (MPI usage) is far from negligible compared to the actual computation.

\end{document}
